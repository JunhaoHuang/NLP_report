\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\citation{chowdhary2020natural}
\citation{Artificial}
\citation{Traditional}
\citation{deepp}
\citation{large}
\citation{liu2017deep,you2019attentionxml}
\citation{Tabobao}
\citation{Nass}
\citation{0recommendation}
\citation{machine}
\citation{data}
\citation{grandini2020metrics}
\citation{domain}
\citation{zhang2010understanding}
\citation{raw}
\citation{prabhu2018parabel,Bonsai,pecos,you2019attentionxml}
\citation{prabhu2018parabel}
\citation{forced}
\citation{you2019attentionxml}
\citation{deeptree}
\HyPL@Entry{0<</S/D>>}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{intro}{{I}{1}{Introduction}{section.1}{}}
\citation{improving}
\citation{,mulity}
\citation{NEURIPS2019_9e6a921f}
\citation{Wiki}
\citation{Amazon-670}
\citation{bbabbar2019data}
\citation{babbar2019data}
\citation{yen2017ppdsparse}
\citation{yen2016pd}
\citation{prabhu2014fastxml}
\citation{weston2013label}
\citation{prabhu2018parabel}
\citation{bhatia2015sparse}
\citation{yu2014large}
\citation{yen2017ppdsparse}
\citation{liu2017deep}
\citation{you2019attentionxml}
\citation{devlin2018bert}
\citation{chang2020taming}
\citation{chang2019x}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related works}{2}{section.2}\protected@file@percent }
\newlabel{sec:related:work}{{II}{2}{Related works}{section.2}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {\mbox  {II-}0a}One-vs-all OVA Approach}{2}{paragraph.2.0.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\numberline {\mbox  {II-}0b}Tree-based Approach}{2}{paragraph.2.0.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\numberline {\mbox  {II-}0c}Embedding-based Approach}{2}{paragraph.2.0.0.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\numberline {\mbox  {II-}0d}Deep Learning Approach}{2}{paragraph.2.0.0.4}\protected@file@percent }
\citation{you2019attentionxml}
\citation{prabhu2018parabel}
\citation{prabhu2018parabel}
\@writefile{toc}{\contentsline {paragraph}{\numberline {\mbox  {II-}0e}Transformer Approach}{3}{paragraph.2.0.0.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}Methodology}{3}{section.3}\protected@file@percent }
\newlabel{sec:method}{{III}{3}{Methodology}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Problem Formulation and Overview}{3}{subsection.3.1}\protected@file@percent }
\newlabel{subsec:overview}{{\mbox  {III-A}}{3}{Problem Formulation and Overview}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}Building a PLT for label clustering}{3}{subsection.3.2}\protected@file@percent }
\newlabel{equ:probability_plt}{{1}{3}{Building a PLT for label clustering}{equation.3.1}{}}
\newlabel{alg:plt_construction}{{1}{3}{Building a PLT for label clustering}{algocf.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces The shallow and wide PLT construction}}{3}{algocf.1}\protected@file@percent }
\citation{you2019attentionxml}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The overall structure of LightAttention}}{4}{figure.1}\protected@file@percent }
\newlabel{fig:structure}{{1}{4}{The overall structure of LightAttention}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Shallow and wide probabilistic label tree for label clustering}}{4}{figure.2}\protected@file@percent }
\newlabel{fig:tree_struct}{{2}{4}{Shallow and wide probabilistic label tree for label clustering}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-C}}Learning LightAttention and Generative Cooperative Networks}{4}{subsection.3.3}\protected@file@percent }
\newlabel{subsec:GCN}{{\mbox  {III-C}}{4}{Learning LightAttention and Generative Cooperative Networks}{subsection.3.3}{}}
\citation{pennington2014glove}
\citation{niu2021review}
\newlabel{equ:generator_loss}{{2}{5}{Learning LightAttention and Generative Cooperative Networks}{equation.3.2}{}}
\newlabel{equ:label_candidate}{{3}{5}{Learning LightAttention and Generative Cooperative Networks}{equation.3.3}{}}
\newlabel{equ:label_embedding}{{4}{5}{Learning LightAttention and Generative Cooperative Networks}{equation.3.4}{}}
\newlabel{equ:discriminator}{{5}{5}{Learning LightAttention and Generative Cooperative Networks}{equation.3.5}{}}
\newlabel{equ:disc_loss}{{6}{5}{Learning LightAttention and Generative Cooperative Networks}{equation.3.6}{}}
\newlabel{equ:total_loss}{{7}{5}{Learning LightAttention and Generative Cooperative Networks}{equation.3.7}{}}
\newlabel{alg:gcn}{{2}{5}{Learning LightAttention and Generative Cooperative Networks}{algocf.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Dynamic negative labels generator for LightAttention}}{5}{algocf.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-D}}Attention-Aware deep model}{5}{subsection.3.4}\protected@file@percent }
\newlabel{subsec:attention_deep_model}{{\mbox  {III-D}}{5}{Attention-Aware deep model}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-D}1}Text representation layer}{5}{subsubsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-D}2}BiLSTM layer}{5}{subsubsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-D}3}Multi-label attention}{5}{subsubsection.3.4.3}\protected@file@percent }
\citation{liu2017deep}
\citation{Amazon-670}
\citation{chalkidis-etal-2019-large}
\citation{Wiki}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Structure of BiLSTM}}{6}{figure.3}\protected@file@percent }
\newlabel{fig:BiLSTM}{{3}{6}{Structure of BiLSTM}{figure.3}{}}
\newlabel{equ:mlattention}{{8}{6}{Multi-label attention}{equation.3.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-D}4}Fully connected layer and output layer}{6}{subsubsection.3.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-D}5}Hidden bottleneck layer}{6}{subsubsection.3.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {IV}Implementation and Results}{6}{section.4}\protected@file@percent }
\newlabel{Experiments}{{IV}{6}{Implementation and Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-A}}Dataset}{6}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\numberline {\mbox  {IV-A}0a}Amazon-670K}{6}{paragraph.4.1.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\numberline {\mbox  {IV-A}0b}EUR-Lex}{6}{paragraph.4.1.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\numberline {\mbox  {IV-A}0c}Wiki10-31}{6}{paragraph.4.1.0.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Hyperparameters used in experiments, practical computation time and model size}}{7}{table.1}\protected@file@percent }
\newlabel{hyper-parameter}{{I}{7}{Hyperparameters used in experiments, practical computation time and model size}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Result comparisons of LightAttention and AttentionXML over three benchmarks.}}{7}{table.2}\protected@file@percent }
\newlabel{DatasetTab}{{II}{7}{Result comparisons of LightAttention and AttentionXML over three benchmarks}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Train Loss Between LightAttention (Green) and AttentionXML (Red)}}{7}{figure.4}\protected@file@percent }
\newlabel{fig:loss}{{4}{7}{Train Loss Between LightAttention (Green) and AttentionXML (Red)}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-B}}Experimental Evaluation Measures}{7}{subsection.4.2}\protected@file@percent }
\newlabel{P}{{9}{7}{Experimental Evaluation Measures}{equation.4.9}{}}
\citation{AttentionXML}
\citation{LightXML}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces  Performance of variant number of trees in LightAttention}}{8}{table.3}\protected@file@percent }
\newlabel{NumTrees}{{III}{8}{Performance of variant number of trees in LightAttention}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-C}}\textbf  {Experimental Settings}}{8}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-D}}Performance comparison}{8}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-E}}The Impact of PLTs }{8}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-F}}Computation Time and Model Size}{8}{subsection.4.6}\protected@file@percent }
\bibstyle{IEEEtran}
\bibdata{ref}
\bibcite{chowdhary2020natural}{1}
\bibcite{Artificial}{2}
\bibcite{Traditional}{3}
\bibcite{deepp}{4}
\bibcite{large}{5}
\bibcite{liu2017deep}{6}
\bibcite{you2019attentionxml}{7}
\bibcite{Tabobao}{8}
\bibcite{Nass}{9}
\bibcite{0recommendation}{10}
\bibcite{machine}{11}
\bibcite{data}{12}
\bibcite{grandini2020metrics}{13}
\bibcite{domain}{14}
\bibcite{zhang2010understanding}{15}
\bibcite{raw}{16}
\bibcite{prabhu2018parabel}{17}
\bibcite{Bonsai}{18}
\bibcite{pecos}{19}
\bibcite{forced}{20}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusions}{9}{section.5}\protected@file@percent }
\newlabel{sec:conclusions}{{V}{9}{Conclusions}{section.5}{}}
\@writefile{toc}{\contentsline {section}{References}{9}{section*.1}\protected@file@percent }
\bibcite{deeptree}{21}
\bibcite{improving}{22}
\bibcite{mulity}{23}
\bibcite{Wiki}{24}
\bibcite{Amazon-670}{25}
\bibcite{bbabbar2019data}{26}
\bibcite{babbar2019data}{27}
\bibcite{yen2017ppdsparse}{28}
\bibcite{yen2016pd}{29}
\bibcite{prabhu2014fastxml}{30}
\bibcite{weston2013label}{31}
\bibcite{bhatia2015sparse}{32}
\bibcite{yu2014large}{33}
\bibcite{devlin2018bert}{34}
\bibcite{chang2020taming}{35}
\bibcite{chang2019x}{36}
\bibcite{pennington2014glove}{37}
\bibcite{niu2021review}{38}
\bibcite{chalkidis-etal-2019-large}{39}
\bibcite{AttentionXML}{40}
\bibcite{LightXML}{41}
\gdef \@abspage@last{10}
