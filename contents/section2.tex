\section{Related work} \label{sec:related:work}
% 
Many works have achieved better performance on Extreme Multi-label. Generally, the approaches can be broadly categorized into five types, one-vs-all OVA approach, tree-based approach, embedding-based approach, deep learning approach and transformer approach.

\paragraph{One-vs-all OVA Approach} 
One-vs-all (OVA) Approach is a heuristic technique for multi-class classification utilizing binary classification algorithms that categorizes multi-class dataset into multiple binary problem. The meaningful contribution is prediction accuracy improvement. But the framework computational size and speed are the major challenges. Several works have tried overcome above disadvantages. The prior research DiSoEMC(Distributed Sparse Machines) \cite{bbabbar2019data} is  the first work to attempts scaling up one-to-one paradigm in extreme multi-label classification problems by parallel training speed-up. ProXML \cite{babbar2019data} modified DiSEMC and focus on long-tail label prediction. Similarity, PPDSparse \cite{yen2017ppdsparse} and PD-Sparse \cite{yen2016pd} use dual sparsity to accelerate training and prediction.

\paragraph{Tree-based Approach} 
Tree based method is a solution in computational issue. In addition, the well known advantages are less training and prediction time by recursively splitting the labels or features. For instance, FastXML is a  classifier for extreme multi-label classification that uses a nDCG-based ranking loss function \cite{prabhu2014fastxml}. It learns a hierarchy structure over the feature space rather than the label space. Similarity, the label partition can use Gini index to evaluate the performance \cite{weston2013label}. The Parabel \cite{prabhu2018parabel} recursively splitting the labels into two balanced groups produces each label tree. If nodes have less than M labels, they become leaves and are not partitioned further. The leaf nodes have linear 1-vsAll classifiers, one for each label in the leaf, that have only been trained on samples that have at least one leaf node label.
Negative examples for training a label's classifier come from other labels in the same leaf as the provided label.
\paragraph{Embedding-based Approach} 
Embedding models employ a low-rank representation for the label matrix in order to a low-dimensional search for label similarity. Embedding-based approaches suppose the label matrix space can be represented by a low-dimensional latent space with comparable latent representations for related labels. For example, SLEEC (Sparse Local Embedding for Extreme Classification ) \cite{bhatia2015sparse} reduce the number of labels by embedding labels onto low dimensional space. Typically, SLEEC uses k-nearest neighbor clusters labels into small groups. In  addition, The \cite{yu2014large} introduces a standard empirical risk minimization framework by using various loss functions and regularization. However, embedding-based models generally perform worse than sparse one-vs-all techniques like PPDSparse\cite{yen2017ppdsparse} to achieve equivalent computational speedups, which might be attributed to the inefficiency of the label representation structure.

\paragraph{Deep Learning Approach} 
With the development of neural network architecture, many deep learning model have shown better improvement in XMC problems. XML-CNN \cite{liu2017deep} is the first work to implement deep neural network in XMC. It learns text representation by forwarding training to CNN networks. XML-CNN also contains a hidden layer to project text features onto low dimensional space in order to reduce the model computational size. However, unlike basic multi-label classification, XML-CNN only utilizes a simple fully connected layer to score all labels with binary entropy loss, making it difficult to deal with big label sets. AttentionXML \cite{you2019attentionxml} uses a probabilistic label tree (PLT) that can deal millions of labels instead of a simple fully connected layer for label scoring. For a single dataset, it must train on many models. AttentionXML solves this problem by multiplying the weight of the current layer model by the weight of its upper layer model, allowing the model to converge fast.

\paragraph{Transformer Approach} 
The NLP proposes a new concept: pre-training then fine-tuning. BERT \cite{devlin2018bert} is one of the pioneering efforts whose pre-training targets include token prediction and following sentence prediction tasks. On the other hand, Transformer model outperforms existing state-of-the-art after pre-training on large-scale unsupervised corpora like Wikipedia and BookCorpus. X-transformer \cite{chang2020taming} is the first pre-trained model implementation on XMC problems. Compared with AttentionXML, X-transformer has higher accuracy. But there are two major shortcomings, firstly, the computational size of model. Secondly, negative labels sampling reduces the prediction accuracy. X-BERT \cite{chang2019x} learns the label representations from  the label and the input text. The process for fine-tuning BERT models to capture the contextual link between input text and the generated label clusters is the important component of X-BERT. An ensemble of multiple BERT models trained on heterogeneous label clusters yields best final model.

