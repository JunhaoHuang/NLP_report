\section{Related works} \label{sec:related:work}
% 
Many works have achieved better performance on XMTC tasks. Generally, the approaches can be broadly categorized into five types, one-vs-all OVA approach, tree-based approach, embedding-based approach, deep learning approach and transformer approach.

\paragraph{One-vs-all OVA Approach} 
One-vs-all (OVA) Approach is a heuristic technique for multi-class classification utilizing binary classification algorithms that categorizes multi-class dataset into multiple binary problem. The meaningful contribution is prediction accuracy improvement. But the framework computational size and speed are the major challenges. Several works have tried overcome above disadvantages. The prior research DiSoEMC(Distributed Sparse Machines) \cite{bbabbar2019data} is  the first work to attempts scaling up one-to-one paradigm in extreme multi-label classification problems by parallel training speed-up. ProXML \cite{babbar2019data} modified DiSEMC and focus on long-tail label prediction. Similarity, PPDSparse \cite{yen2017ppdsparse} and PD-Sparse \cite{yen2016pd} use dual sparsity to accelerate training and prediction.

\paragraph{Tree-based Approach} 
Tree based method is a solution in computational issue. In addition, the well known advantages are less training and prediction time by recursively splitting the labels or features. For instance, FastXML uses ranking loss function to modify the classifier\cite{prabhu2014fastxml}. Particularly,it constructs a hierarchy structure over the feature space. Similarity, the label partition can use Gini index to evaluate the performance \cite{weston2013label}. The Parabel \cite{prabhu2018parabel} recursively splitting the labels into two balanced groups produces each label tree. If nodes have less than M labels, they become leaves and are not partitioned further. The leaf nodes have linear classifiers.Negative examples for training a label's classifier come from other labels in the same leaf as the provided label.
\paragraph{Embedding-based Approach} 
Embedding models employ a low-rank matrix for the label representation in order to a low-dimensional search for label similarity.For example, SLEEC (Sparse Local Embedding for Extreme Classification ) \cite{bhatia2015sparse} embeds labels onto low dimensional space in order to get low dimensional and dense martix. Typically, SLEEC uses k-nearest neighbor clusters labels into small groups. In  addition, The \cite{yu2014large} introduces a standard empirical risk minimization framework by using various loss functions and regularization. However, embedding-based models generally perform worse than sparse one-vs-all techniques like PPDSparse\cite{yen2017ppdsparse}. The reason for this problem should be the embedding representation structure.

\paragraph{Deep Learning Approach} 
As neural network architectures have evolved, many deep learning models have shown better improvements in XMC problems. XML-CNN \cite{liu2017deep} is the first work to implement deep neural networks on XMC. It gets text representations by forwarding training with CNN network. XML-CNN has s a hidden layer for projecting text features into a low-dimensional space to reduce the computational size. However, unlike the basic multi-label classification, XML-CNN utilizes  a simple fully-connected layer to score all labels with binary entropy loss, making it difficult to handle large label sets. AttentionXML \cite{you2019attentionxml} uses a probabilistic label tree (PLT) that can handle millions of labels, rather than a simple fully-connected layer, to perform label scoring.

\paragraph{Transformer Approach} 
The NLP proposes a new concept: pre-training then fine-tuning. BERT \cite{devlin2018bert} is the pioneer work whose pre-training targets include token prediction and following sentence prediction tasks. On the other hand, Transformer model outperforms existing state-of-the-art after pre-training on large-scale unsupervised corpora like Wikipedia and BookCorpus. X-transformer \cite{chang2020taming} is the first pre-trained model implementation on XMC problems. Compared with AttentionXML, X-transformer has higher accuracy. But there are two major shortcomings, firstly, the computational size of model. Secondly, negative labels sampling reduces the prediction accuracy. X-BERT \cite{chang2019x} learns the label representations from  the label and the input text. The process for fine-tuning BERT to exploit the contextual relations between input text. Besides, the generated label clusters is the important component of X-BERT. The best final model is trained on multiple BERT heterogeneous clusters.

