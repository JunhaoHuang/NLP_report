\section{Introduction}\label{intro}
% NLP and 引出 MTC task
Natural Language Processing (NLP), as a multidisciplinary field of linguistics, computer science, and artificial intelligence, has received extensive research attention due to the rapid development of machine learning methods recently. Compared to the traditional statistical NLP, deep neural network shows impressive suitability in NLP, which can achieve state-of-the-art results in many natural language tasks, especially for extreme multi-label text classification (XMTC) task. Due to recent development of the Internet, the application scenarios of MTC are prevalent in our daily life, such as item categorization in Taobao, news annotation, recommendation system, website tagging, and so on. The objective of XMTC is to label a given text with multiple possible labels from an extremely large-scale label set. This is distinct from the multi-class classification, which only tag each given text one single label. 

During the era of big data, the data scale of the information systems has experienced massive growth. The Internet service providers have to handle millions of labels and samples. Text classification for those extreme scale datasets becomes an extremely difficult NLP task. The difficulties of XMTC mainly comes from two parts. First, the training phrase of XMTC for extreme datasets requires significant computational challenges, which brings many troubles in developing effective deep learning model for developers. Besides, millions of labels (tail labels) experience the lack of positive samples, thus decreasing the training effectiveness on these tail labels.

According to the representation of the input datasets and labels, we can classify the existing works into two categories: (1) Utilize the semantic features of the labels or input datasets as the training datasets. This type of strategy requires extra effort to extra semantic features such as bag-of-words (BOW) from the input texts. (2) Directly use the raw input text as the training dataset. This method directly feed the training model with sparse vectors of the texts. 

In order to improve the training accuracy for tail labels, many works \cite{prabhu2018parabel,Bonsai,pecos,you2019attentionxml} proposed label partitions or probabilistic label trees (PLTs) strategies to exploit the relations within labels. Among which, Parabel \cite{prabhu2018parabel} utilizes the BOW information of the labels, which ignores the context information of long-distance dependency of words. Besides, Parabel's tree-based methods for labels classifies a lot of dissimilar labels into a single cluster, which further decreases the classification accuracy for tail labels. AttentionXML \cite{you2019attentionxml} addresses the existing problems in Parabel by using raw text information and a more shallow and wide PLT model for training. Using raw text information together with a Bidirectional-LSTM (BiLSTM) model takes more features especially the context information among words into consideration. 