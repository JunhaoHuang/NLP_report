\begin{figure*}[htb]
	\centering
	\includegraphics[width=0.6\textwidth]{structure.png}
	\caption{The overall structure of LightAttention}
	\label{fig:structure}
\end{figure*}

\begin{figure*}[htb]
	\centering
	\includegraphics[width=0.6\textwidth]{tree.png}
	\caption{Shallow and wide probabilistic label tree for label clustering}
	\label{fig:tree_struct}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology} \label{sec:method}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Problem Formulation and Overview}\label{subsec:overview}


The XMTC task is to find multiple relevant labels for each given raw text. The training set $\{x_i,y_i\}_{i=1}^{N}$, where $x_i$ is the origin source, $y_i\in \{0,1\}^L$ is the relevant label of $x_i$ and $y_i$ is a multi-hot $L$-dimensional vectors. The goal of XMTC is to train a model that can better simulate the function $f(x_i)\in R^L$, such that $f$ output a high score for the $l$-th label $y_{il}$ if $y_{il}$ is relevant to the text $x_i$. However, one significant problem in XMTC is that $L$ can be up to millions, which makes it impossible to directly train a model on $L$-dimensional vectors due to the high workload. Therefore, in order to perform multi-label classification over tens of thousands or millions of label set, we first need to construct a probabilistic label tree (PLT) to divide massive number of labels into smaller label clusters, thus accelerating the process of label classification. After we build a PLT for label clustering, we then train an attention-aware deep learning model LightAttention by combining the BiLSTM, multi-label attention layer and generative cooperative networks for negative label sampling. The whole structure of the LightAttention is listed in \autoref{fig:structure}.

\subsection{Building a PLT for label clustering}
For the PLT construction, we follow the method shown in AttentionXML \cite{you2019attentionxml}, which constructs a shallow and wide PLT from the original deep PLT presented in Parabel \cite{prabhu2018parabel}. Their PLT construction algorithm first utilizes the KMeans (K=2) algorithm to generate a hierarchical deep PLT. After that, they presents an algorithm to split down the layer of the deep PLT due to the reason that a deep PLT would result in slower performance. 

The so-called PLT is to construct a tree with $L$ leaves. Here, each leaf represent a unique label. Suppose that there exists a text $x$, for this given text, we assign each node in the PLT a value $z_n\in \{0,1\}$. $z_n=0$ means that the children node of $n$ doesn't have any relations with the given text $x$. Otherwise, $z_n=0$ indicates that there exists at least one children node of $n$ is relevant to $x$. An example of the shallow and wide PLT can be found in \autoref{fig:tree_struct}, during which the nodes in purple represent $z_n=1$ and the nodes in green indicate $z_n=0$. PLT evaluates the conditional probability that each node $n$'s relevance with $x$ by computing $P\left(z_{n} \mid z_{P a(n)}=1, x\right)$, where $Pa(n)$ is the parent node of $n$. Then, the probability that how each node $n$ is relevant to $x$ can be simply computed with \autoref{equ:probability_plt}.

\begin{equation}
	P\left(z_{n}=1 \mid x\right)=\prod_{i \in P a t h(n)} P\left(z_{i}=1 \mid z_{P a(i)}=1, x\right)
	\label{equ:probability_plt}
\end{equation}
\noindent The $Path(n)$ refers to the nodes appeared between node $n$ and root.

A PLT has two important parameters: tree height $H$ and cluster size $M$. If these two parameters are too big, then the overall performance of the PLT would be very slow. Therefore, we follow the method in AttentionXML and build a shallow and wide PLT $T_H$, reducing both the tree height $H$ and wide $M$. The overall procedure of this algorithm is shown in \autoref{alg:plt_construction}. This algorithm takes $T_0$, which is built with the Parabel method \cite{prabhu2018parabel}, as input, and it performs the compression function $H$ times over the parents of leaves $S_0$. This algorithm first select $c$-th ancestor nodes as $S_l$. Then, remove the nodes between $S_{l-1}$ and $S_l$ to reduce the overall number of nodes. Finally, reset the tree based on the new nodes. After these steps, a shallow and wide tree $T_H$ with smaller height $H$ and smaller wide $M$ can be obtained.
 
\begin{algorithm}[thb]
    \caption{The shallow and wide PLT construction\label{alg:plt_construction}}
    \begin{algorithmic}[1]
        \REQUIRE{Labels of training texts \(\{y_i\}_{i=1}^{N}\); Initial PLT $T_0$; \(K=2^c,H\)}
        \ENSURE{A shallow and wide PLT \(T\)}
        \STATE{Initialize parent nodes of leaves $S_0$}
		\FOR{$l \in [1,H]$}
		\IF{$l < H$}
		\STATE{$S_l \leftarrow$ \{$c$-th ancestor node $n$ of nodes in $S_{l-1}$\}}
		\ELSE{}
		\STATE{$S_l \leftarrow$ \{the root of $T_0$\}}
		\ENDIF{}
		\STATE{$T_l \leftarrow T_{l-1}$}
		\FOR{nodes $n\in S_l$}
		\FOR{nodes $n^{\prime}\in S_{l-1}$ and node $n$ is the ancestor of $n^{\prime}$ in T}
		\STATE{$Pa(n^{\prime})\leftarrow n$}
		\ENDFOR{}
		\ENDFOR{}
		\ENDFOR{}
        \STATE{}
		\Return{\(T_H\)}
    \end{algorithmic}
\end{algorithm}

\subsection{Learning LightAttention and Generative Cooperative Networks}\label{subsec:GCN}
% how GCN is used in the model
After we construct a PLT, we need to train a deep model among the PLT. For a deep PLT, the nodes near the bottom layer is very difficult since the labels. Instead of training the model for all nodes together, AttentionXML presents a level-wise and top-down manner for each level of nodes. They define AttentionXML$_d$ as the training procedure of training the $d$-level candidate nodes $g(x)$ for given sample $x$. The candidate nodes is selected by first sortting the $(d-1)$-level nodes by $z_n$ (from positive to negative) and the scores of each nodes obtained in AttentionXML$_{d-1}$. Then, the children nodes of the top $k$ nodes at the $(d-1)$-level is the training candidate $g(x)$. AttentionXML$_{1}$ can be directly computed for the root nodes. 

During the original AttentionXML, the candidate nodes $g(x)$ obtained by the top $k$ function may consist of both positive and negative nodes. The negative nodes obtained here are all static negative label samples and sometimes, there may be no negative label samples at all. The trained model would overfit due to the lack of negative samples. The convergence of the model with static negative samples would also be difficult due to the high similarity between the positive and negative labels in the original AttentionXML. To address this problem in AttentionXML, we adopt the strategy presented in \cite{you2019attentionxml} and deploy Generative Cooperative Networks (GCN) to generate negative label samples dynamically so that the training procedure can distinguish more negative samples from positive samples, reducing the overfitting issue in AttentionXML.

Specifically, the GCN for LightAttention consists of the two components: label generator, discriminator. These two components work as a cooperative networks. The label generator first generates negative labels and delivers the the negative samples into the discriminator to learn the difference between negative and positive label samples. Here, the discriminator can achieve better label representation with the help of large dynamic negative samples.

\textbf{Generator.} The generator is a fully connected layer $G(e)=\sigma\left(W_{g} e+b_{g}\right)$ that output a $K$-dimensional vectors representing the scores of all $K$ label clusters. Top $b$ label clusters with highest scores are choosen at the generation seed of labels. Then, all these labels together with positive labels are added as the training label sets so that the deep model can be trained to recognize the postive and negative labels. The generator loss is computed as \autoref{equ:generator_loss}, during which $y_g\in {0,1}^K$ represents the multi-hot label representation of label cluster for $x$.
\begin{equation}
	\begin{aligned}
	\mathcal{L}_{g}\left(G(e), y_{g}\right)=& \sum_{i=0}^{K}\left(1-y_{g}^{i}\right)\left(-\log \left(1-G(e)^{i}\right)\right)+\\
	& y_{g}^{i}\left(-\log \left(G(e)^{i}\right)\right)
	\end{aligned}
	\label{equ:generator_loss}
\end{equation}

Instead of training each level of nodes separately as AttentionXML, we treat the whole label clusters as the labels we sample. The generator needs to regenerate negative labels for the same training texts, so that the discriminator can better distinguish positive and negative labels. The label candidates generated by the generator in LightAttention is generated as \autoref{equ:label_candidate}:
\begin{equation}
	S_{g}=\left\{l_{i}: i \in\left\{i: g_{c}\left(l_{i}\right) \in G(e)\right\}\right\}
	\label{equ:label_candidate}
\end{equation}
\noindent where $g_c$ helps to map labels to its clusters and $l_i$ denotes the $i$-th label. During the training phrase, all positive labels are added to $S_g$ as the whole label clusters.

\textbf{Discriminator.} After we obtain the label candidates $S_g$ containing both positive and negative labels, we have to convert the label candidates into label embedding $M$ for each label in $S_g$. This is denoted as \autoref{equ:label_embedding}. 
\begin{equation}
	M=\left[E_{i}: i \in\left\{i: S_{g}\right\}\right]
	\label{equ:label_embedding}
\end{equation}
\noindent where $E_i \in R^b$ is the modifiable $b$ dimension embedding of $i$-th label and $E \in R^{L\times b}$ is the embedding matrix storing all label embedding. This matrix is initialized randomly and will be updated repeatedly during the training procedure. 

\begin{equation}
	D(e, M)=\sigma\left(M \sigma\left(W_{h} e+b_{h}\right)\right)
	\label{equ:discriminator}
\end{equation}

The discriminator, as shown in \autoref{equ:discriminator}, is deployed into a hidden bottleneck layer in the deep model. During \autoref{equ:discriminator}, $W_h \in R^{b\times 5k}$ and $b_h \in R^b$ are the weight of the hidden bottleneck layer. The goal of the discriminator $D(e, M )$ is to detect the difference between positive and negative labels generated by the generator. The training target of $D(e, M )$ is $y_d$ where $y_d^i = 0$ if $S_g^i$ is a positive label and $y_d^i = 1$ if $S_g^i$ is a negative label. Therefore, the loss for discriminator phrase can be computed as \autoref{equ:disc_loss}.
\begin{equation}
	\begin{aligned}
		\mathcal{L}_{d}\left(D(e, M), y_{d}\right)=& \sum_{i=0}^{K}\left(1-y_{d}^{i}\right)\left(-\log \left(1-D(e, M)^{i}\right)\right)+\\
		& y_{d}^{i}\left(-\log \left(D(e, M)^{i}\right)\right)
		\end{aligned}
	\label{equ:disc_loss}
\end{equation}
\noindent The total loss function of the whole training model is the combination of the generator and discriminator, as shown in \autoref{equ:total_loss}. 
\begin{equation}
	\mathcal{L}=\mathcal{L}_{g}+\mathcal{L}_{d}
	\label{equ:total_loss}
\end{equation}

The whole procedures of generative cooperative netowrks are shown in \autoref{alg:gcn}. 
\begin{algorithm}[thb]
    \caption{Dynamic negative labels generator for LightAttention\label{alg:gcn}}
    \begin{algorithmic}[1]
        \REQUIRE{Labels of training texts \(\{X,Y\}=\{(x_i,y_i)\}_{i=1}^{N}\), semantic features of the training text $\hat{X}$}
        \ENSURE{Label embedding $M$}
        \STATE{Label clusters $C$ generation using $\hat{X}, Y$}
		\STATE{Discriminator $D$ initialization over cluster $C$}
		\STATE{Label embedding $E$, generator $G$ initialization over cluster $C$}
		\STATE{Get $m$ samples from {$X,Y$}: {$X_{batch},Y_{batch}$}}
		\STATE{Get text embedding $\hat{h}$ from BiLSTM}
		\FOR{$l \in [1,m]$}
			\STATE{Generate label clusters $S_{generated}$ using $G(\hat{h_l})$}
			\STATE{Select negative labels $S_{neg}$ using $S_{generated},C$}
			\STATE{Delete positive labels from $S_{neg}$}
		\ENDFOR{}
		\STATE{Generate positive labels $S_{pos}$ using $Y_{batch}$}
		\FOR{$l \in [1,m]$}
			\STATE{Generate label embedding $M$ using $S_{pos},S_{neg}$}
			% \STATE{Compute label scores using $D(\hat{h_l},M)$}
		\ENDFOR{}
        \STATE{}
		\Return{\(M\)}
    \end{algorithmic}
\end{algorithm}


\subsection{Attention-Aware deep model}\label{subsec:attention_deep_model}
As shown in \autoref{fig:structure},the deep model of LightAttention can be briefly divided into six layers: text representation section, BiLSTM section, multi attention setction, fully connected layer, hidden bottleneck layer, and output layer.

\subsubsection{Text representation layer} The input text of our model is raw text with length $\hat{T}$. The famous 300-dimensional GloVe \cite{pennington2014glove} word embedding representation is used in our model.

\subsubsection{BiLSTM layer} BiLSTM, abbreviation of Bidirectional long short-term memory, is one of the recursive neural networks (RNN). The overall structure of BiLSTM is shown in \autoref{fig:BiLSTM}. It is equipped with two LSTM layers: one forward LSTM and one backward LSTM. Each LSTM is used to capture either forward or backward context information in a raw text, thus providing much more adequate context information for later usage. The use of BiLSTM is the major reason that we can accept raw text as input in our model. The output $\hat{h_t}$ ($t$ is the time step) of the BiLSTM in our model is the combination of both forward and backward outputs.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.4\textwidth]{BiLSTM.png}
	\caption{Structure of BiLSTM}
	\label{fig:BiLSTM}
\end{figure}


% \subsubsection{Generative cooperative networks} 

\subsubsection{Multi-label attention}
Attention mechanism \cite{niu2021review} assigns the most  highest weights by a weighted combination of all encoded input vectors. In XMTC, the most relevant context that each label captures would be totally different. We adopts the multi-label attention shown in AttentionXML, which detect the intensive parts of text for multiple labels by computing:
\begin{equation}
	\hat{\mathbf{m}}_{j}=\sum_{i=1}^{\hat{T}} \alpha_{i j} \hat{\mathbf{h}}_{i}, \quad \alpha_{i j}=\frac{e^{\hat{\mathbf{h}}_{i} \hat{\mathbf{w}}_{j}}}{\sum_{t=1}^{\hat{T}} e^{\hat{\mathbf{h}}_{t} \hat{\mathbf{w}}_{j}}}
	\label{equ:mlattention}
\end{equation}
\noindent where $\hat{m_j}\in R^{2\hat{N}}$ is the output of multi-label attention, $\alpha_{ij}$ is the normalized coefficient of $\hat{h_i}$ and $\hat{w_j}\in R^{2\hat{N}}$ is the attention parameters.

\subsubsection{Fully connected layer and output layer}
LightAttention contains two fully connected layers and one output layer. 

\subsubsection{Hidden bottleneck layer}
We conduct the hidden bottleneck layer between the fully connected layer and output layer as in \cite{liu2017deep}. The hidden bottleneck layer also enables the generator and discriminator to concentrate variance  of text representations. The generator and the discriminator focuses on fine-grained textual information coarse-grained textual information respectively.

% \subsection{Loss }