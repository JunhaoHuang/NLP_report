\begin{table*}[tbp]
  \centering
  \caption{Hyperparameters used in experiments, practical computation time and model size}\label{hyper-parameter}
  \begin{adjustbox}{width=0.7 \textwidth,center}  
    \begin{tabular}{c|cccc|ccc}
    \hline
    Datasets & E     & B     & $\bar{N}$     & $\bar{N} _{fc}$   & Train & Test  & Model Size \\
    \hline
    EUR-Lex & 30    & 40    & 512   & 512   & 0.74  & 2.03  & 0.58 \\
    Wiki10-31K & 30    & 40    & 512   & 512   & 1.68  & 5.65  & 0.68 \\
    AmazonCat-13K & 10    & 200   & 512   & 512   & 20.5  & 1.53  & 0.95 \\
    \hline
    \end{tabular}%
  \end{adjustbox}
\end{table*}%

 % Table generated by Excel2LaTeX from sheet 'Sheet1'
% \begin{threeparttable}
\begin{table*}[tbp]
    \centering
    \caption{Result comparisons of LightAttention and AttentionXML over three benchmarks.}\label{DatasetTab}
    \begin{adjustbox}{width=0.7 \textwidth,center}    
    \begin{tabular}{c|c|c|c|c}
      \hline
      Dataset & \textcolor[rgb]{ .02,  .388,  .757}{P@1=N@1} & \textcolor[rgb]{ .02,  .388,  .757}{P@3} & \textcolor[rgb]{ .02,  .388,  .757}{P@5} &  Model \\
      \hline
      \multirow{2}[4]{*}{Amazon-670K} & \textbf{86.56} & \textbf{72.98} & 60.21 & AttentionXML \\
  \cline{2-5}          & 86.53 & 71.58 & \textbf{61.92} & LightAttention \\
      \hline
      \multirow{2}[4]{*}{EUR-Lex} & 47.5  & 41.56 & 35.32 & AttentionXML \\
  \cline{2-5}          & \textbf{48.6} & \textbf{43.21} & \textbf{38.54} & LightAttention \\
      \hline
      \multirow{2}[4]{*}{Wiki10-31k} & \textbf{87.4} & 77.4  & 68.21 & AttentionXML \\
  \cline{2-5}          & 86.5  & \textbf{77.6} & \textbf{68.35} & LightAttention \\
      \hline
      \end{tabular}%
    \end{adjustbox}
  \end{table*}%
%  \end{threeparttable}
  
  \begin{figure*}[h] 
\centering
\includegraphics[width=1\textwidth]{loss.pdf} 
\caption{Train Loss Between LightAttention (Green) and AttentionXML (Red)}
\label{fig:loss}
\end{figure*}
  
  \begin{table*}[tbp]
    \centering
    \caption{ Performance of variant number of trees in LightAttention}\label{NumTrees}
    \begin{adjustbox}{width=0.8 \textwidth,center}  
    \begin{tabular}{c|ccc|ccc|ccc}
      \hline
            & \multicolumn{3}{c}{Amazon-670K} & \multicolumn{3}{c}{EUR-Lex} & \multicolumn{3}{c}{Wiki10-31K} \\
      \hline
      Trees & {P@1} & {P@2} & {P@5} & {P@1} & {P@2} & {P@5} & {P@1} & {P@2} & {P@5} \\ \hline
      1     & 82.95 & 65.18 & 54.56 & 44.35 & 36.15 & 36.15 & 80.57 & 72.18 & 59.61 \\
      2     & 84.21 & 68.15 & 58.91 & 45.15 & 39.51 & 37.02 & 82.51 & 73.5  & 60.58 \\
      3     & 85.13 & 69.18 & 60.14 & 46.89 & 41.1  & 37.24 & 84.6  & 75.31 & 65.31 \\
      4     & \textbf{86.53} & \textbf{71.53} & \textbf{61.92} & \textbf{48.6} & \textbf{43.21} & \textbf{38.54} & \textbf{86.5} & \textbf{77.6} & \textbf{68.35} 
      \\
      \hline
      \end{tabular}%
    \end{adjustbox}
  \end{table*}%
\section{Implementation and Results}

In experiments section, we mainly introduce the experimental datasets, experimental evaluation measures, platform configuration and experimental settings and performance comparison results.
\label{Experiments}

\subsection{Dataset}
Extreme multi-label task is a changeling NLP problem that aims to learn a classifier that can predict text label in an extremely  set of labels set. In order to obtain a classifier with good results, an efficient data structure is essential. We choose three most famous extreme multi-label text classification (XMTC) for our experiments: three open source datasets (range from 4K to 30K): EUR-Lex, Amazon-670K and Wiki10-31K (shown in \ref{hyper-parameter}).In this table, $E$ stands for the number of epoch; $B$ presents the batch size; $\bar{N} _{fc}$ shows the hidden unit size of the LSTM; $\bar{N}$ stand for the hidden unit size of fully connected layers; $H$ stands for The height of PLT (excluding the root and leaves);$M$ shows the maximum cluster size; K: The parameters of the compress process, and here we set $M = K = 2c$; And finally $C$ demostrates the number of parents of candidate nodes.
% Table generated by Excel2LaTeX from sheet 'Result'

\paragraph{Amazon-670K} Amazon-670K\cite{Amazon-670} provides a huge dataset for evaluating the performance of extreme multi-label text classification (XMTC) including evaluation code and judgment metrics.it is mainly composed of web page information and company product information, which is an excellent training dataset for XMTC tasks with complex types and large data volumes.
There are 135,909 bow feature dimensionalities,	670,091 labels,	490,449 train dataset, and 153,025 test dataset. To keep balance in the distribution of data, Amazon-670K also provides over 3.99 dates per label and 5.45 labels per data.
Amazon-670K is very perfect for pre-processing the data. For small-scale datasets, the authors have provided the complete data in one file. We provide separate files for the training and test splits, which contain the indexes of the points in the training and test sets. Each column of the split file contains a separate split. For large-scale datasets, the authors provide a train and a test split into two separate files.Amazon-670K also provides the option to download pre-computed (e.g., a packet of words) features or raw text, and the tokenization that can be used to create a packet of word representation may vary across datasets.



\paragraph{EUR-Lex} EUR-Lex is a famous and official website for European Union laws and other public documents, and it has published 24 official languages in the European Union. A large amount of data exists on this website for English-speaking nature languages, and based on this data,Ilias Chalkidis and  Manos Fergadiotis \cite{chalkidis-etal-2019-large} proposed EUR-Lex dataset. The Eur-Lex dataset is suitable for LMTC task and zero-short/few-short learning,  this allowed user to bypass BERTâ€™s maximum text length limit and fine-tune BERT, obtaining the best results in all but zero-shot learning cases.

EUR-Lex contains 57 thousand legal instruments with a length of 750 words. Each instrument includes 4 mainly zones: the header, which imports the title and the abstract of this legal instrument body; the recitals, which are the background of legal references and also contain the relevant laws  and legal texts; the main body, the most important part of the document, which focuses on the organization in articles; and the attachments, Attachments about the instrument, such as evidence, photos, minutes of meetings, court dates, etc. Since LMTC task requires large size documents to be input as smaller units (e.g., sentences), but in EUR-Lux they are chapters, so the author preprocessed the raw text of the dataset. they consider the header,the recitals, the main body, and the attachments as separate sections. In addition to that, authors alse split EUR-Lex into training dataset, development dataset and testing dataset, which are have 45 thousand instruments, 6 thousand instruments and 6 thousand instruments.

EUR-Lex provide 4,271 labels and divide them into frequent labels (746), few-short labels (3,362) and zero-shot labels (163).
\paragraph{Wiki10-31} Wiki10-31K \cite{Wiki} is also a well-known NLP dataset for XMTC task training. It was created based on the content of a Wiki, resulting in its inclusion of a lot of content, ranging from astronomy down to geography. The excessive variety of data also makes it difficult to fit and any model will have difficulty achieving good data results on it.

Wiki10-31K mainly contains instances (20,762), attributes (132,876), labels 930,938, and labelsets (20693), but the density of Wiki10-31K only is 0.0006, that's why many famous algorithms is hard to get an excellent result.Wiki10-31 was used in our experiments to test the robustness of the algorithm in extreme cases.

\subsection{Experimental Evaluation Measures}
In our experiments, we choose $P@k$ (k is Precision) as evaluation metrics for our method named LightAttention and AttentionXML, $P@k$ is broadly used as an evaluation indicator for NLP XMTC (shown in \ref{P}).
\begin{align}
    P@k = \frac{1}{k}\sum_{l=1}^{k}y_{rank(l)}   \label{P} 
\end{align}

Note $y\in \left \{ 0,1 \right \} ^{L} $ stand for ground turth binary vector, and $rank(l)$ points out the toppest l of the predicted label. Another well-known metric is $N@k$ (stand for normalized discounted Cumulative Gain at k point). In this equasion, we know that $P@1 == N@1$.
 And then we evaluate algorithm's performance by $N@k$, though prove process, we can calculate that the trend of $N@k$ keep the same speed as $P@k$, so we can omit the results of N@k in the main text due to space limitation.
 \subsection{\textbf{Experimental Settings}}
 
 LightAttention is an NLP algorithm focused on XMTC, so we compared it with the most representative XMTC method, called AttentionXML.  In order to increase the authority and feasibility of the experiments we did several experiments in three data sets Amazon-670K, EUR-Lex, and Wiki10-31K respectively, and used their average result values to obtain the final results.
 To ensure the accuracy of the experiments, we perform a simple pre-processing of the datasets. For each experiments, the most frequent token in the training set are extracted and formed into a new efficient training set (no more than 500,000).
And for EUR-Lex and Wiki10-31K, the fine-tune of word embeddings  process is conducted in the training process.

For more efficient training and more accurate prediction, we slice the data into units of 500. After the embedding layer, we use dropout = 0.3 to avoid overfitting of the neural network, and after the BiLSTM layer, dropout = 0.4. In addition to this, LightAttention has a learning rate of 1e-3. We also use SWA to improve performance.
 


\subsection{Performance comparison}

LightAttention is based on the evolution of AttentionXML \cite{AttentionXML} and add Label Embedding Structure from LightXML \cite{LightXML}, so here we focus on comparing the two algorithms. First we tested the experimental results on three different datasets (Amazon-670K, EUR-Lex and Wiki10-31k) and compared the loss functions of the training process of the two codes. That is, we demonstrate the superiority of the two algorithms in terms of both process and results.

Table \ref{DatasetTab} A stands for the performance between LightAttention and AttentionXML by $P@k$ evaluation metric, and we test those methods in EUR-Lex, Amazon-670K, and Wiki10-31K several times.
Because we import subsections from LightXML, LightXML has an excellent ability to handle extremely less label text classification tasks. The experimental results are clearly evident in EUR-Lex dataset. LightAttention has better experimental results in $P@1$, $P@2$ and $P@3$, compared to AttentionXML, LightAttention improves accuracy by 3.22\% in the $P@2$ branch of EUR-Lex. And the experiments in others dataset shows that LightAttention has the same dataset processing capabilities as AttentionXML when dealing with common datasets. 
Another point, as shown in the \autoref{fig:loss}, is that LightAttention has a lower loss function than AttentionXML during training, which also proves that LightAttention has an outstanding ability when dealing with some extreme data.

\subsection{The Impact of PLTs }
Since the number of PLTs can substantially affect the accuracy of the algorithm, in this section, we tested the effect of the number of PLTs on LightAttention, and the specific experimental results are shown below.
We tested the influence of LightAttention with different number of PLTs on the model performance through extensive experiments, as shown in Table \ref{NumTrees}
Through these experiments, we can clearly see that more branches can substantially improve the accuracy of prediction. However, using more trees requires more time for training and prediction data and more storage space. So it is a tradeoff between performance and time cost.
\subsection{Computation Time and Model Size}

The efficiency of the model calculation depends on the performance of the GPU, and the server configuration we use is shown below:

CPU:E5-2670; GPU: 3070; Computer system: Ubuntu 18.04.


Computation time and model memory are crucial for NLP models. Low time and less memory represent a task for NLP models not only for high accuracy but also for efficiency. The following content will  give detail information about the computation time and model size of LightAtteition. For AttentionXML, it uses a large neural network model and high-dimensional linear classification that causes a large model size and high computational complexity. We did not compare LightAttention with AttentionXML because AttentionXML is difficult to replicate and has poor efficiency.

As shown in Table \ref{hyper-parameter}, with different batchsize and different dataset, will yield different models as well as time. The longest dataset is AmazonCat-13K, with a training time of 20.5 hours and a testing time of 1.53 hours, and the largest model is 0.95GB.

