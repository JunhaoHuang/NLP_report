\section{\textbf{Experiments}}

In this section, we mainly introduce the experimental datasets, experimental evaluation measures, platform configuration and experimental settings and performance comparison results.
\label{Experiments}

\subsection{\textbf{Dataset}}
Extreme multi-label learning is a very complex NLP problem that aims to train a classifier that can automatically label a new data point from an extremely large set of labels and provide the most relevant subset of labels.In order to obtain a classifier with good results, an efficient data structure is essential. We choose three most famous extreme multi-label text classification (XMTC) for our experiments: three large-scale datasets (range from 4K to 30K): EUR-Lex, Amazon-670K and Wiki10-31K (shown in \ref{hyper-parameter}). E is the number of epochs, B represents the batch size.
% Table generated by Excel2LaTeX from sheet 'Result'
\begin{table*}[htbp]
  \centering
  \caption{Hyperparameters used in experiments, practical computation time and model size}\label{hyper-parameter}
    \begin{tabular}{c|cccc|ccc}
    \hline
    Datasets & E     & B     & N     & Nfc   & Train & Test  & Model Size \\
    \hline
    EUR-Lex & 30    & 40    & 512   & 512   & 0.74  & 2.03  & 0.58 \\
    Wiki10-31K & 30    & 40    & 512   & 512   & 1.68  & 5.65  & 0.68 \\
    AmazonCat-13K & 10    & 200   & 512   & 512   & 20.5  & 1.53  & 0.95 \\
    \hline
    \end{tabular}%
  \label{tab:addlabel}%
\end{table*}%





\paragraph{Amazon-670K} Amazon-670K\cite{Amazon-670} provides a huge dataset for evaluating the performance of extreme multi-label text classification (XMTC) including evaluation code and judgment metrics.it is mainly composed of web page information and company product information, which is an excellent training dataset for XMTC tasks with complex types and large data volumes.
There are 135,909 bow feature dimensionalities,	670,091 labels,	490,449 train dataset, and 153,025 test dataset. To keep balance in the distribution of data, Amazon-670K also provides over 3.99 dates per label and 5.45 labels per data.
Amazon-670K is very perfect for pre-processing the data. For small-scale datasets, the authors have provided the complete data in one file. We provide separate files for the training and test splits, which contain the indexes of the points in the training and test sets. Each column of the split file contains a separate split. For large-scale datasets, the authors provide a train and a test split into two separate files.Amazon-670K also provides the option to download pre-computed (e.g., a packet of words) features or raw text, and the tokenization that can be used to create a packet of word representation may vary across datasets.



\paragraph{EUR-Lex} EUR-Lex is a famous and official website for European Union laws and other public documents, and it has published 24 official languages in the European Union. A large amount of data exists on this website for English-speaking nature languages, and based on this data,Ilias Chalkidis and  Manos Fergadiotis \cite{chalkidis-etal-2019-large} proposed EUR-Lex dataset. The Eur-Lex dataset is suitable for LMTC task and zero-short/few-short learning,  this allowed user to bypass BERTâ€™s maximum text length limit and fine-tune BERT, obtaining the best results in all but zero-shot learning cases.

EUR-Lex contains 57 thousand legal instruments with a length of 750 words. Each instrument includes 4 mainly zones: the header, which imports the title and the abstract of this legal instrument body; the recitals, which are the background of legal references and also contain the relevant laws  and legal texts; the main body, the most important part of the document, which focuses on the organization in articles; and the attachments, Attachments about the instrument, such as evidence, photos, minutes of meetings, court dates, etc. Since LMTC task requires large size documents to be input as smaller units (e.g., sentences), but in EUR-Lux they are chapters, so the author preprocessed the raw text of the dataset. they consider the header,the recitals, the main body, and the attachments as separate sections. In addition to that, authors alse split EUR-Lex into training dataset, development dataset and testing dataset, which are have 45 thousand instruments, 6 thousand instruments and 6 thousand instruments.

EUR-Lex provide 4,271 labels and divide them into frequent labels (746), few-short labels (3,362) and zero-shot labels (163).
\paragraph{Wiki10-31} Wiki10-31K \cite{Wiki} is also a well-known NLP dataset for XMTC task training. It was created based on the content of a Wiki, resulting in its inclusion of a lot of content, ranging from astronomy down to geography. The excessive variety of data also makes it difficult to fit and any model will have difficulty achieving good data results on it.

Wiki10-31K mainly contains instances (20,762), attributes (132,876), labels 930,938, and labelsets (20693), but the density of Wiki10-31K only is 0.0006, that's why many famous algorithms is hard to get an excellent result.Wiki10-31 was used in our experiments to test the robustness of the algorithm in extreme cases.

\subsection{\textbf{Experimental Evaluation Measures}}
In our experiments, we choose $P@k$ (k is Precision) as evaluation metrics for our method named LightAttention and AttentionXML, $P@k$ is broadly used as an evaluation indicator for NLP XMTC (shown in \ref{P}).
\begin{align}
    P@k = \frac{1}{k}\sum_{l=1}^{k}y_{rank(l)}   \label{P} 
\end{align}

Note $y\in \left \{ 0,1 \right \} ^{L} $ stand for ground turth binary vector, and $rank(l)$ points out the toppest l of the predicted label. Another well-known metric is $N@k$ (stand for normalized discounted Cumulative Gain at k point). In this equasion, we know that $P@1 == N@1$.
 And then we evaluate algorithm's performance by $N@k$, though prove process, we can calculate that the trend of $N@k$ keep the same speed as $P@k$, so we can omit the results of N@k in the main text due to space limitation.
 \subsection{\textbf{Experimental Settings}}
 
 LightAttention is an NLP algorithm focused on XMTC, so we compared it with the most representative XMTC method called AttentionXML. And In order to increase the authority and feasibility of the experiments we did several experiments in three data sets Amazon-670K, EUR-Lex, and Wiki10-31K respectively, and used their average result values to obtain the final results.
 To ensure the accuracy of the experiments, we perform a simple pre-processing of the datasets. For each dataset, the most frequent words in the training set are extracted and formed into a new efficient training set (no more than 500,000).
And for EUR-Lex and Wiki10-31K, we fine-tune the word embeddings during the training process.

For more efficient training and more accurate prediction, we slice the data into units of 500. After the embedding layer, we use dropout = 0.3 to avoid overfitting of the neural network, and after the BiLSTM layer, dropout = 0.4. In addition to this, LightAttention has a learning rate of 1e-3. We also use SWA to improve performance.
 
 % Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Add caption}\label{DatasetTab}
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    Dataset & \textcolor[rgb]{ .02,  .388,  .757}{P@1=N@1} & \textcolor[rgb]{ .02,  .388,  .757}{P@3} & \textcolor[rgb]{ .02,  .388,  .757}{P@5} &  \\
    \hline
    \multirow{2}[4]{*}{Amazon-670K} & \textbf{86.56} & \textbf{72.98} & 60.21 & AttentionXML \\
\cline{2-5}          & 86.53 & 71.58 & \textbf{61.92} & LightAttention \\
    \hline
    \multirow{2}[4]{*}{EUR-Lex} & 47.5  & 41.56 & 35.32 & AttentionXML \\
\cline{2-5}          & \textbf{48.6} & \textbf{43.21} & \textbf{38.54} & LightAttention \\
    \hline
    \multirow{2}[4]{*}{Wiki10-31k} & \textbf{87.4} & 77.4  & 68.21 & AttentionXML \\
\cline{2-5}          & 86.5  & \textbf{77.6} & \textbf{68.35} & LightAttention \\
    \hline
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%

\subsection{\textbf{Performance comparison}}

\begin{figure*}[h] 
\centering
\includegraphics[width=0.9\textwidth]{loss.pdf} 
\caption{Train Loss Between LightAttention (Green) and AttentionXML (Red)}
\label{fig:loss}
\end{figure*}

Table \ref{DatasetTab} A stands for the performance between LightAttention and AttentionXML by $P@k$ evaluation metric, and we test those methods in EUR-Lex, Amazon-670K, and Wiki10-31K several times.
Because we import subsections from LightXML, LightXML has an excellent ability to handle extremely less label text classification tasks. The experimental results are clearly evident in EUR-Lex dataset. LightAttention has better experimental results in $P@1$, $P@2$ and $P@3$, compared to AttentionXML, LightAttention improves accuracy by 3.22\% in the $P@2$ branch of EUR-Lex. And the experiments in others dataset shows that LightAttention has the same dataset processing capabilities as AttentionXML when dealing with common datasets. 
Another point, as shown in the \autoref{fig:loss}, is that LightAttention has a lower loss function than AttentionXML during training, which also proves that LightAttention has an outstanding ability when dealing with some extreme data.

\subsection{\textbf{The Number of PLTs Impact}}
We tested the influence of LightAttention with different number of PLTs on the model performance through extensive experiments, as shown in Table \ref{NumTree}
Through these experiments, we can clearly see that more branches can substantially improve the accuracy of prediction. However, using more trees requires more time for training and prediction data and more storage space. So it is a tradeoff between performance and time cost.


% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table*}[htbp]
  \centering
  \caption{ Performance of variant number of trees in LightAttention}\label{NumTrees}
    \begin{tabular}{|c|ccc|ccc|ccc|}
    \hline
          & \multicolumn{3}{c}{Amazon-670K} & \multicolumn{3}{c}{EUR-Lex} & \multicolumn{3}{c|}{Wiki10-31K} \\
    \hline
    \hline
    Trees & {P@1} & {P@2} & {P@5} & {P@1} & {P@2} & {P@5} & {P@1} & {P@2} & {P@5} \\ \hline
    1     & 82.95 & 65.18 & 54.56 & 44.35 & 36.15 & 36.15 & 80.57 & 72.18 & 59.61 \\
    2     & 84.21 & 68.15 & 58.91 & 45.15 & 39.51 & 37.02 & 82.51 & 73.5  & 60.58 \\
    3     & 85.13 & 69.18 & 60.14 & 46.89 & 41.1  & 37.24 & 84.6  & 75.31 & 65.31 \\
    4     & \textbf{86.53} & \textbf{71.53} & \textbf{61.92} & \textbf{48.6} & \textbf{43.21} & \textbf{38.54} & \textbf{86.5} & \textbf{77.6} & \textbf{68.35} 
    \\
    \hline
    \end{tabular}%
  \label{tab:addlabel}%
\end{table*}%
